# Optional: to enable NVIDIA GPU support when available, run the helper script `scripts/start.sh` (Linux/macOS) or `scripts/start.ps1` (Windows).
# The helper will detect NVIDIA GPUs and automatically include `docker-compose.gpu.yml` if present.
# Do NOT edit this file if you want automated detection; see scripts/ for more details.
#
# Note: the `ai-file-organizer` service is configured under the Compose profile
# `ai-file-organizer`, so it will NOT be created when you run `docker compose up -d`
# by default. To enable it, either pass `--profile ai-file-organizer` to the
# compose command or set the environment variable `COMPOSE_PROFILES=ai-file-organizer`.
# The helper scripts (`scripts/start.sh` and `scripts/start.ps1`) also support
# enabling the app via the START_APP environment variable or the `--with-app` flag.

services:
  ai-file-organizer:
    # Start only when profile `ai-file-organizer` is enabled
    profiles:
      - ai-file-organizer
    build: .
    image: ai-file-organizer:latest
    container_name: ai-file-organizer
    volumes:
      - ./input:/input
      - ./output:/output
      - ./config.yml:/app/config.yml
    environment:
      - CONFIG_PATH=/app/config.yml
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY:-}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT:-}
      # Use a smaller model for tests (default: 'tinyllama')
      # - OLLAMA_MODEL=${OLLAMA_MODEL:-tinyllama}
      # For larger model testing, comment above and uncomment below:
      - OLLAMA_MODEL=llama2
    depends_on:
      - ollama

  # Local LLM service using Ollama for testing
  ollama:
    image: ollama/ollama:latest
    container_name: ai-file-organizer-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama  # Persist Ollama model cache
    environment:
      - OLLAMA_HOST=0.0.0.0

volumes:
  ollama-data:
