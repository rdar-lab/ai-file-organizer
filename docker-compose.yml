services:
  ai-file-organizer:
    build: .
    image: ai-file-organizer:latest
    container_name: ai-file-organizer
    volumes:
      - ./input:/input
      - ./output:/output
      - ./config.yml:/app/config.yml
    environment:
      - CONFIG_PATH=/app/config.yml
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY:-}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT:-}
      # Use a smaller model for tests (default: 'tinyllama')
      # - OLLAMA_MODEL=${OLLAMA_MODEL:-tinyllama}
      # For larger model testing, comment above and uncomment below:
      - OLLAMA_MODEL=llama2
    depends_on:
      - ollama

  # Local LLM service using Ollama for testing
  ollama:
    image: ollama/ollama:latest
    container_name: ai-file-organizer-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama  # Persist Ollama model cache
    environment:
      - OLLAMA_HOST=0.0.0.0

volumes:
  ollama-data:
